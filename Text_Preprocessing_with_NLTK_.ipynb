{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOovLQw5HYwiGns5OQBVtL1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HananehKoushki/NLP-Assignment/blob/master/Text_Preprocessing_with_NLTK_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tokenization**"
      ],
      "metadata": {
        "id": "uxsrEQ2kkWU2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1lcWcn9aci6d",
        "outputId": "eb9d58a9-a172-4c31-a9d3-3f3cb4b5fcc5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "# Download the Punkt tokenizer\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The following paragraph has 533 words (Microsoft Word count).\n",
        "\n",
        "paragraph = \"\"\"If we assume as the historians do that great men lead humanity to the attainment of certain ends- the greatness of Russia or of France, the balance of power in Europe, the diffusion of the ideas of the Revolution general progress or anything else- then it is impossible to explain the facts of history without introducing the conceptions of chance and genius.\n",
        "If the aim of the European wars at the beginning of the nineteenth century had been the aggrandizement of Russia, that aim might have been accomplished without all the preceding wars and without the invasion. If the aim wag the aggrandizement of France, that might have been attained without the Revolution and without the Empire. If the aim was the dissemination of ideas, the printing press could have accomplished that much better than warfare. If the aim was the progress of civilization, it is easy to see that there are other ways of diffusing civilization more expedient than by the destruction of wealth and of human lives.\n",
        "Why did it happen in this and not in some other way?\n",
        "Because it happened so! ‘Chance created the situation; genius utilized it,’ says history.\n",
        "But what is chance? What is genius?\n",
        "The words chance and genius do not denote any really existing thing and therefore cannot be defined. Those words only denote a certain stage of understanding of phenomena. I do not know why a certain event occurs; I think that I cannot know it; so I do not try to know it and I talk about chance. I see a force producing effects beyond the scope of ordinary human agencies; I do not understand why this occurs and I talk of genius.\n",
        "To a herd of rams, the ram the herdsman drives each evening into a special enclosure to feed and that becomes twice as fat as the others must seem to be a genius. And it must appear an astonishing conjunction of genius with a whole series of extraordinary chances that this ram, who instead of getting into the general fold every evening goes into a special enclosure where there are oats- that this very ram, swelling with fat, is killed for meat.\n",
        "But the rams need only cease to suppose that all that happens to them happens solely for the attainment of their sheepish aims; they need only admit that what happens to them may also have purposes beyond their ken, and they will at once perceive a unity and coherence in what happened to the ram that was fattened. Even if they do not know for what purpose they are fattened, they will at least know that all that happened to the ram did not happen accidentally, and will no longer need the conceptions of chance or genius.\n",
        "Only by renouncing our claim to discern a purpose immediately intelligible to us, and admitting the ultimate purpose to be beyond our ken, may we discern the sequence of experiences in the lives of historic characters and perceive the cause of the effect they produce (incommensurable with ordinary human capabilities), and then the words chance and genius become superfluous.\n",
        "We need only confess that we do not know the purpose of the European convulsions and that we know only the facts- that is, the murders, first in France, then in Italy, in Africa, in Prussia, in Austria, in Spain, and in Russia- and that the movements from the west to the east and from the east to the west form the essence and purpose of these events, and not only shall we have no need to see exceptional ability and genius in Napoleon and Alexander, but we shall be unable to consider them to be anything but like other men, and we shall not be obliged to have recourse to chance for an explanation of those small events which made these people what they were, but it will be clear that all those small events were inevitable.\n",
        "By discarding a claim to knowledge of the ultimate purpose, we shall clearly perceive that just as one cannot imagine a blossom or seed for any single plant better suited to it than those it produces, so it is impossible to imagine any two people more completely adapted down to the smallest detail for the purpose they had to fulfill, than Napoleon and Alexander with all their antecedents.\"\"\""
      ],
      "metadata": {
        "id": "RoQwPmOCdwv1"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizing sentences\n",
        "# Split the paragraph into sentences\n",
        "\n",
        "sentences = nltk.sent_tokenize(paragraph)\n",
        "sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50PnvaYWhqWd",
        "outputId": "1b4ca78c-7513-492d-d90d-233f1c172ee4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['If we assume as the historians do that great men lead humanity to the attainment of certain ends- the greatness of Russia or of France, the balance of power in Europe, the diffusion of the ideas of the Revolution general progress or anything else- then it is impossible to explain the facts of history without introducing the conceptions of chance and genius.',\n",
              " 'If the aim of the European wars at the beginning of the nineteenth century had been the aggrandizement of Russia, that aim might have been accomplished without all the preceding wars and without the invasion.',\n",
              " 'If the aim wag the aggrandizement of France, that might have been attained without the Revolution and without the Empire.',\n",
              " 'If the aim was the dissemination of ideas, the printing press could have accomplished that much better than warfare.',\n",
              " 'If the aim was the progress of civilization, it is easy to see that there are other ways of diffusing civilization more expedient than by the destruction of wealth and of human lives.',\n",
              " 'Why did it happen in this and not in some other way?',\n",
              " 'Because it happened so!',\n",
              " '‘Chance created the situation; genius utilized it,’ says history.',\n",
              " 'But what is chance?',\n",
              " 'What is genius?',\n",
              " 'The words chance and genius do not denote any really existing thing and therefore cannot be defined.',\n",
              " 'Those words only denote a certain stage of understanding of phenomena.',\n",
              " 'I do not know why a certain event occurs; I think that I cannot know it; so I do not try to know it and I talk about chance.',\n",
              " 'I see a force producing effects beyond the scope of ordinary human agencies; I do not understand why this occurs and I talk of genius.',\n",
              " 'To a herd of rams, the ram the herdsman drives each evening into a special enclosure to feed and that becomes twice as fat as the others must seem to be a genius.',\n",
              " 'And it must appear an astonishing conjunction of genius with a whole series of extraordinary chances that this ram, who instead of getting into the general fold every evening goes into a special enclosure where there are oats- that this very ram, swelling with fat, is killed for meat.',\n",
              " 'But the rams need only cease to suppose that all that happens to them happens solely for the attainment of their sheepish aims; they need only admit that what happens to them may also have purposes beyond their ken, and they will at once perceive a unity and coherence in what happened to the ram that was fattened.',\n",
              " 'Even if they do not know for what purpose they are fattened, they will at least know that all that happened to the ram did not happen accidentally, and will no longer need the conceptions of chance or genius.',\n",
              " 'Only by renouncing our claim to discern a purpose immediately intelligible to us, and admitting the ultimate purpose to be beyond our ken, may we discern the sequence of experiences in the lives of historic characters and perceive the cause of the effect they produce (incommensurable with ordinary human capabilities), and then the words chance and genius become superfluous.',\n",
              " 'We need only confess that we do not know the purpose of the European convulsions and that we know only the facts- that is, the murders, first in France, then in Italy, in Africa, in Prussia, in Austria, in Spain, and in Russia- and that the movements from the west to the east and from the east to the west form the essence and purpose of these events, and not only shall we have no need to see exceptional ability and genius in Napoleon and Alexander, but we shall be unable to consider them to be anything but like other men, and we shall not be obliged to have recourse to chance for an explanation of those small events which made these people what they were, but it will be clear that all those small events were inevitable.',\n",
              " 'By discarding a claim to knowledge of the ultimate purpose, we shall clearly perceive that just as one cannot imagine a blossom or seed for any single plant better suited to it than those it produces, so it is impossible to imagine any two people more completely adapted down to the smallest detail for the purpose they had to fulfill, than Napoleon and Alexander with all their antecedents.']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizing words\n",
        "# Split the paragraph into words\n",
        "\n",
        "words = nltk.word_tokenize(paragraph)\n",
        "words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IIJGlrRvjiLE",
        "outputId": "ae7ed588-cb88-4b77-c61b-b74b27172fc1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['If',\n",
              " 'we',\n",
              " 'assume',\n",
              " 'as',\n",
              " 'the',\n",
              " 'historians',\n",
              " 'do',\n",
              " 'that',\n",
              " 'great',\n",
              " 'men',\n",
              " 'lead',\n",
              " 'humanity',\n",
              " 'to',\n",
              " 'the',\n",
              " 'attainment',\n",
              " 'of',\n",
              " 'certain',\n",
              " 'ends-',\n",
              " 'the',\n",
              " 'greatness',\n",
              " 'of',\n",
              " 'Russia',\n",
              " 'or',\n",
              " 'of',\n",
              " 'France',\n",
              " ',',\n",
              " 'the',\n",
              " 'balance',\n",
              " 'of',\n",
              " 'power',\n",
              " 'in',\n",
              " 'Europe',\n",
              " ',',\n",
              " 'the',\n",
              " 'diffusion',\n",
              " 'of',\n",
              " 'the',\n",
              " 'ideas',\n",
              " 'of',\n",
              " 'the',\n",
              " 'Revolution',\n",
              " 'general',\n",
              " 'progress',\n",
              " 'or',\n",
              " 'anything',\n",
              " 'else-',\n",
              " 'then',\n",
              " 'it',\n",
              " 'is',\n",
              " 'impossible',\n",
              " 'to',\n",
              " 'explain',\n",
              " 'the',\n",
              " 'facts',\n",
              " 'of',\n",
              " 'history',\n",
              " 'without',\n",
              " 'introducing',\n",
              " 'the',\n",
              " 'conceptions',\n",
              " 'of',\n",
              " 'chance',\n",
              " 'and',\n",
              " 'genius',\n",
              " '.',\n",
              " 'If',\n",
              " 'the',\n",
              " 'aim',\n",
              " 'of',\n",
              " 'the',\n",
              " 'European',\n",
              " 'wars',\n",
              " 'at',\n",
              " 'the',\n",
              " 'beginning',\n",
              " 'of',\n",
              " 'the',\n",
              " 'nineteenth',\n",
              " 'century',\n",
              " 'had',\n",
              " 'been',\n",
              " 'the',\n",
              " 'aggrandizement',\n",
              " 'of',\n",
              " 'Russia',\n",
              " ',',\n",
              " 'that',\n",
              " 'aim',\n",
              " 'might',\n",
              " 'have',\n",
              " 'been',\n",
              " 'accomplished',\n",
              " 'without',\n",
              " 'all',\n",
              " 'the',\n",
              " 'preceding',\n",
              " 'wars',\n",
              " 'and',\n",
              " 'without',\n",
              " 'the',\n",
              " 'invasion',\n",
              " '.',\n",
              " 'If',\n",
              " 'the',\n",
              " 'aim',\n",
              " 'wag',\n",
              " 'the',\n",
              " 'aggrandizement',\n",
              " 'of',\n",
              " 'France',\n",
              " ',',\n",
              " 'that',\n",
              " 'might',\n",
              " 'have',\n",
              " 'been',\n",
              " 'attained',\n",
              " 'without',\n",
              " 'the',\n",
              " 'Revolution',\n",
              " 'and',\n",
              " 'without',\n",
              " 'the',\n",
              " 'Empire',\n",
              " '.',\n",
              " 'If',\n",
              " 'the',\n",
              " 'aim',\n",
              " 'was',\n",
              " 'the',\n",
              " 'dissemination',\n",
              " 'of',\n",
              " 'ideas',\n",
              " ',',\n",
              " 'the',\n",
              " 'printing',\n",
              " 'press',\n",
              " 'could',\n",
              " 'have',\n",
              " 'accomplished',\n",
              " 'that',\n",
              " 'much',\n",
              " 'better',\n",
              " 'than',\n",
              " 'warfare',\n",
              " '.',\n",
              " 'If',\n",
              " 'the',\n",
              " 'aim',\n",
              " 'was',\n",
              " 'the',\n",
              " 'progress',\n",
              " 'of',\n",
              " 'civilization',\n",
              " ',',\n",
              " 'it',\n",
              " 'is',\n",
              " 'easy',\n",
              " 'to',\n",
              " 'see',\n",
              " 'that',\n",
              " 'there',\n",
              " 'are',\n",
              " 'other',\n",
              " 'ways',\n",
              " 'of',\n",
              " 'diffusing',\n",
              " 'civilization',\n",
              " 'more',\n",
              " 'expedient',\n",
              " 'than',\n",
              " 'by',\n",
              " 'the',\n",
              " 'destruction',\n",
              " 'of',\n",
              " 'wealth',\n",
              " 'and',\n",
              " 'of',\n",
              " 'human',\n",
              " 'lives',\n",
              " '.',\n",
              " 'Why',\n",
              " 'did',\n",
              " 'it',\n",
              " 'happen',\n",
              " 'in',\n",
              " 'this',\n",
              " 'and',\n",
              " 'not',\n",
              " 'in',\n",
              " 'some',\n",
              " 'other',\n",
              " 'way',\n",
              " '?',\n",
              " 'Because',\n",
              " 'it',\n",
              " 'happened',\n",
              " 'so',\n",
              " '!',\n",
              " '‘',\n",
              " 'Chance',\n",
              " 'created',\n",
              " 'the',\n",
              " 'situation',\n",
              " ';',\n",
              " 'genius',\n",
              " 'utilized',\n",
              " 'it',\n",
              " ',',\n",
              " '’',\n",
              " 'says',\n",
              " 'history',\n",
              " '.',\n",
              " 'But',\n",
              " 'what',\n",
              " 'is',\n",
              " 'chance',\n",
              " '?',\n",
              " 'What',\n",
              " 'is',\n",
              " 'genius',\n",
              " '?',\n",
              " 'The',\n",
              " 'words',\n",
              " 'chance',\n",
              " 'and',\n",
              " 'genius',\n",
              " 'do',\n",
              " 'not',\n",
              " 'denote',\n",
              " 'any',\n",
              " 'really',\n",
              " 'existing',\n",
              " 'thing',\n",
              " 'and',\n",
              " 'therefore',\n",
              " 'can',\n",
              " 'not',\n",
              " 'be',\n",
              " 'defined',\n",
              " '.',\n",
              " 'Those',\n",
              " 'words',\n",
              " 'only',\n",
              " 'denote',\n",
              " 'a',\n",
              " 'certain',\n",
              " 'stage',\n",
              " 'of',\n",
              " 'understanding',\n",
              " 'of',\n",
              " 'phenomena',\n",
              " '.',\n",
              " 'I',\n",
              " 'do',\n",
              " 'not',\n",
              " 'know',\n",
              " 'why',\n",
              " 'a',\n",
              " 'certain',\n",
              " 'event',\n",
              " 'occurs',\n",
              " ';',\n",
              " 'I',\n",
              " 'think',\n",
              " 'that',\n",
              " 'I',\n",
              " 'can',\n",
              " 'not',\n",
              " 'know',\n",
              " 'it',\n",
              " ';',\n",
              " 'so',\n",
              " 'I',\n",
              " 'do',\n",
              " 'not',\n",
              " 'try',\n",
              " 'to',\n",
              " 'know',\n",
              " 'it',\n",
              " 'and',\n",
              " 'I',\n",
              " 'talk',\n",
              " 'about',\n",
              " 'chance',\n",
              " '.',\n",
              " 'I',\n",
              " 'see',\n",
              " 'a',\n",
              " 'force',\n",
              " 'producing',\n",
              " 'effects',\n",
              " 'beyond',\n",
              " 'the',\n",
              " 'scope',\n",
              " 'of',\n",
              " 'ordinary',\n",
              " 'human',\n",
              " 'agencies',\n",
              " ';',\n",
              " 'I',\n",
              " 'do',\n",
              " 'not',\n",
              " 'understand',\n",
              " 'why',\n",
              " 'this',\n",
              " 'occurs',\n",
              " 'and',\n",
              " 'I',\n",
              " 'talk',\n",
              " 'of',\n",
              " 'genius',\n",
              " '.',\n",
              " 'To',\n",
              " 'a',\n",
              " 'herd',\n",
              " 'of',\n",
              " 'rams',\n",
              " ',',\n",
              " 'the',\n",
              " 'ram',\n",
              " 'the',\n",
              " 'herdsman',\n",
              " 'drives',\n",
              " 'each',\n",
              " 'evening',\n",
              " 'into',\n",
              " 'a',\n",
              " 'special',\n",
              " 'enclosure',\n",
              " 'to',\n",
              " 'feed',\n",
              " 'and',\n",
              " 'that',\n",
              " 'becomes',\n",
              " 'twice',\n",
              " 'as',\n",
              " 'fat',\n",
              " 'as',\n",
              " 'the',\n",
              " 'others',\n",
              " 'must',\n",
              " 'seem',\n",
              " 'to',\n",
              " 'be',\n",
              " 'a',\n",
              " 'genius',\n",
              " '.',\n",
              " 'And',\n",
              " 'it',\n",
              " 'must',\n",
              " 'appear',\n",
              " 'an',\n",
              " 'astonishing',\n",
              " 'conjunction',\n",
              " 'of',\n",
              " 'genius',\n",
              " 'with',\n",
              " 'a',\n",
              " 'whole',\n",
              " 'series',\n",
              " 'of',\n",
              " 'extraordinary',\n",
              " 'chances',\n",
              " 'that',\n",
              " 'this',\n",
              " 'ram',\n",
              " ',',\n",
              " 'who',\n",
              " 'instead',\n",
              " 'of',\n",
              " 'getting',\n",
              " 'into',\n",
              " 'the',\n",
              " 'general',\n",
              " 'fold',\n",
              " 'every',\n",
              " 'evening',\n",
              " 'goes',\n",
              " 'into',\n",
              " 'a',\n",
              " 'special',\n",
              " 'enclosure',\n",
              " 'where',\n",
              " 'there',\n",
              " 'are',\n",
              " 'oats-',\n",
              " 'that',\n",
              " 'this',\n",
              " 'very',\n",
              " 'ram',\n",
              " ',',\n",
              " 'swelling',\n",
              " 'with',\n",
              " 'fat',\n",
              " ',',\n",
              " 'is',\n",
              " 'killed',\n",
              " 'for',\n",
              " 'meat',\n",
              " '.',\n",
              " 'But',\n",
              " 'the',\n",
              " 'rams',\n",
              " 'need',\n",
              " 'only',\n",
              " 'cease',\n",
              " 'to',\n",
              " 'suppose',\n",
              " 'that',\n",
              " 'all',\n",
              " 'that',\n",
              " 'happens',\n",
              " 'to',\n",
              " 'them',\n",
              " 'happens',\n",
              " 'solely',\n",
              " 'for',\n",
              " 'the',\n",
              " 'attainment',\n",
              " 'of',\n",
              " 'their',\n",
              " 'sheepish',\n",
              " 'aims',\n",
              " ';',\n",
              " 'they',\n",
              " 'need',\n",
              " 'only',\n",
              " 'admit',\n",
              " 'that',\n",
              " 'what',\n",
              " 'happens',\n",
              " 'to',\n",
              " 'them',\n",
              " 'may',\n",
              " 'also',\n",
              " 'have',\n",
              " 'purposes',\n",
              " 'beyond',\n",
              " 'their',\n",
              " 'ken',\n",
              " ',',\n",
              " 'and',\n",
              " 'they',\n",
              " 'will',\n",
              " 'at',\n",
              " 'once',\n",
              " 'perceive',\n",
              " 'a',\n",
              " 'unity',\n",
              " 'and',\n",
              " 'coherence',\n",
              " 'in',\n",
              " 'what',\n",
              " 'happened',\n",
              " 'to',\n",
              " 'the',\n",
              " 'ram',\n",
              " 'that',\n",
              " 'was',\n",
              " 'fattened',\n",
              " '.',\n",
              " 'Even',\n",
              " 'if',\n",
              " 'they',\n",
              " 'do',\n",
              " 'not',\n",
              " 'know',\n",
              " 'for',\n",
              " 'what',\n",
              " 'purpose',\n",
              " 'they',\n",
              " 'are',\n",
              " 'fattened',\n",
              " ',',\n",
              " 'they',\n",
              " 'will',\n",
              " 'at',\n",
              " 'least',\n",
              " 'know',\n",
              " 'that',\n",
              " 'all',\n",
              " 'that',\n",
              " 'happened',\n",
              " 'to',\n",
              " 'the',\n",
              " 'ram',\n",
              " 'did',\n",
              " 'not',\n",
              " 'happen',\n",
              " 'accidentally',\n",
              " ',',\n",
              " 'and',\n",
              " 'will',\n",
              " 'no',\n",
              " 'longer',\n",
              " 'need',\n",
              " 'the',\n",
              " 'conceptions',\n",
              " 'of',\n",
              " 'chance',\n",
              " 'or',\n",
              " 'genius',\n",
              " '.',\n",
              " 'Only',\n",
              " 'by',\n",
              " 'renouncing',\n",
              " 'our',\n",
              " 'claim',\n",
              " 'to',\n",
              " 'discern',\n",
              " 'a',\n",
              " 'purpose',\n",
              " 'immediately',\n",
              " 'intelligible',\n",
              " 'to',\n",
              " 'us',\n",
              " ',',\n",
              " 'and',\n",
              " 'admitting',\n",
              " 'the',\n",
              " 'ultimate',\n",
              " 'purpose',\n",
              " 'to',\n",
              " 'be',\n",
              " 'beyond',\n",
              " 'our',\n",
              " 'ken',\n",
              " ',',\n",
              " 'may',\n",
              " 'we',\n",
              " 'discern',\n",
              " 'the',\n",
              " 'sequence',\n",
              " 'of',\n",
              " 'experiences',\n",
              " 'in',\n",
              " 'the',\n",
              " 'lives',\n",
              " 'of',\n",
              " 'historic',\n",
              " 'characters',\n",
              " 'and',\n",
              " 'perceive',\n",
              " 'the',\n",
              " 'cause',\n",
              " 'of',\n",
              " 'the',\n",
              " 'effect',\n",
              " 'they',\n",
              " 'produce',\n",
              " '(',\n",
              " 'incommensurable',\n",
              " 'with',\n",
              " 'ordinary',\n",
              " 'human',\n",
              " 'capabilities',\n",
              " ')',\n",
              " ',',\n",
              " 'and',\n",
              " 'then',\n",
              " 'the',\n",
              " 'words',\n",
              " 'chance',\n",
              " 'and',\n",
              " 'genius',\n",
              " 'become',\n",
              " 'superfluous',\n",
              " '.',\n",
              " 'We',\n",
              " 'need',\n",
              " 'only',\n",
              " 'confess',\n",
              " 'that',\n",
              " 'we',\n",
              " 'do',\n",
              " 'not',\n",
              " 'know',\n",
              " 'the',\n",
              " 'purpose',\n",
              " 'of',\n",
              " 'the',\n",
              " 'European',\n",
              " 'convulsions',\n",
              " 'and',\n",
              " 'that',\n",
              " 'we',\n",
              " 'know',\n",
              " 'only',\n",
              " 'the',\n",
              " 'facts-',\n",
              " 'that',\n",
              " 'is',\n",
              " ',',\n",
              " 'the',\n",
              " 'murders',\n",
              " ',',\n",
              " 'first',\n",
              " 'in',\n",
              " 'France',\n",
              " ',',\n",
              " 'then',\n",
              " 'in',\n",
              " 'Italy',\n",
              " ',',\n",
              " 'in',\n",
              " 'Africa',\n",
              " ',',\n",
              " 'in',\n",
              " 'Prussia',\n",
              " ',',\n",
              " 'in',\n",
              " 'Austria',\n",
              " ',',\n",
              " 'in',\n",
              " 'Spain',\n",
              " ',',\n",
              " 'and',\n",
              " 'in',\n",
              " 'Russia-',\n",
              " 'and',\n",
              " 'that',\n",
              " 'the',\n",
              " 'movements',\n",
              " 'from',\n",
              " 'the',\n",
              " 'west',\n",
              " 'to',\n",
              " 'the',\n",
              " 'east',\n",
              " 'and',\n",
              " 'from',\n",
              " 'the',\n",
              " 'east',\n",
              " 'to',\n",
              " 'the',\n",
              " 'west',\n",
              " 'form',\n",
              " 'the',\n",
              " 'essence',\n",
              " 'and',\n",
              " 'purpose',\n",
              " 'of',\n",
              " 'these',\n",
              " 'events',\n",
              " ',',\n",
              " 'and',\n",
              " 'not',\n",
              " 'only',\n",
              " 'shall',\n",
              " 'we',\n",
              " 'have',\n",
              " 'no',\n",
              " 'need',\n",
              " 'to',\n",
              " 'see',\n",
              " 'exceptional',\n",
              " 'ability',\n",
              " 'and',\n",
              " 'genius',\n",
              " 'in',\n",
              " 'Napoleon',\n",
              " 'and',\n",
              " 'Alexander',\n",
              " ',',\n",
              " 'but',\n",
              " 'we',\n",
              " 'shall',\n",
              " 'be',\n",
              " 'unable',\n",
              " 'to',\n",
              " 'consider',\n",
              " 'them',\n",
              " 'to',\n",
              " 'be',\n",
              " 'anything',\n",
              " 'but',\n",
              " 'like',\n",
              " 'other',\n",
              " 'men',\n",
              " ',',\n",
              " 'and',\n",
              " 'we',\n",
              " 'shall',\n",
              " 'not',\n",
              " 'be',\n",
              " 'obliged',\n",
              " 'to',\n",
              " 'have',\n",
              " 'recourse',\n",
              " 'to',\n",
              " 'chance',\n",
              " 'for',\n",
              " 'an',\n",
              " 'explanation',\n",
              " 'of',\n",
              " 'those',\n",
              " 'small',\n",
              " 'events',\n",
              " 'which',\n",
              " 'made',\n",
              " 'these',\n",
              " 'people',\n",
              " 'what',\n",
              " 'they',\n",
              " 'were',\n",
              " ',',\n",
              " 'but',\n",
              " 'it',\n",
              " 'will',\n",
              " 'be',\n",
              " 'clear',\n",
              " 'that',\n",
              " 'all',\n",
              " 'those',\n",
              " 'small',\n",
              " 'events',\n",
              " 'were',\n",
              " 'inevitable',\n",
              " '.',\n",
              " 'By',\n",
              " 'discarding',\n",
              " 'a',\n",
              " 'claim',\n",
              " 'to',\n",
              " 'knowledge',\n",
              " 'of',\n",
              " 'the',\n",
              " 'ultimate',\n",
              " 'purpose',\n",
              " ',',\n",
              " 'we',\n",
              " 'shall',\n",
              " 'clearly',\n",
              " 'perceive',\n",
              " 'that',\n",
              " 'just',\n",
              " 'as',\n",
              " 'one',\n",
              " 'can',\n",
              " 'not',\n",
              " 'imagine',\n",
              " 'a',\n",
              " 'blossom',\n",
              " 'or',\n",
              " 'seed',\n",
              " 'for',\n",
              " 'any',\n",
              " 'single',\n",
              " 'plant',\n",
              " 'better',\n",
              " 'suited',\n",
              " 'to',\n",
              " 'it',\n",
              " 'than',\n",
              " 'those',\n",
              " 'it',\n",
              " 'produces',\n",
              " ',',\n",
              " 'so',\n",
              " 'it',\n",
              " 'is',\n",
              " 'impossible',\n",
              " 'to',\n",
              " 'imagine',\n",
              " 'any',\n",
              " 'two',\n",
              " 'people',\n",
              " 'more',\n",
              " 'completely',\n",
              " 'adapted',\n",
              " 'down',\n",
              " 'to',\n",
              " 'the',\n",
              " 'smallest',\n",
              " 'detail',\n",
              " 'for',\n",
              " 'the',\n",
              " 'purpose',\n",
              " 'they',\n",
              " 'had',\n",
              " 'to',\n",
              " 'fulfill',\n",
              " ',',\n",
              " 'than',\n",
              " 'Napoleon',\n",
              " 'and',\n",
              " 'Alexander',\n",
              " 'with',\n",
              " 'all',\n",
              " 'their',\n",
              " 'antecedents',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Stemming**"
      ],
      "metadata": {
        "id": "1nv_HRjTGVDW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download NLTK stopwords corpus\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2dGmcDkikvrb",
        "outputId": "8c0a2b9b-26f7-4664-b8d5-5498179002b9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stemming\n",
        "# This part applies stemming to each sentence in the list `sentences`\n",
        "\n",
        "for i in range(len(sentences)):\n",
        "\n",
        " # Tokenize the sentence (split into individual words)\n",
        "  words = nltk.word_tokenize(sentences[i])\n",
        "\n",
        " # Remove stop words (common words like \"the\", \"a\", \"is\") that don't add much meaning\n",
        " # and apply stemming (reduce words to their base form) to the remaining words\n",
        "  words = [PorterStemmer().stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
        "\n",
        " # Join the stemmed words back into a sentence\n",
        "  sentences[i] = ' '.join(words)"
      ],
      "metadata": {
        "id": "GpSSv80eHrBj"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "-MTlEVVvMSGL",
        "outputId": "db2f8226-9d4f-45cb-c99d-1842c4c7e17b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'if aim european war begin nineteenth centuri aggrandiz russia , aim might accomplish without preced war without invas .'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lemmatization**"
      ],
      "metadata": {
        "id": "N0jq3-_BO0IV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download WordNet resources\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LoSxJW0gKnrD",
        "outputId": "5aa66b3a-ef96-4502-e866-c5e08f42a71f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentence tokenization (split paragraph into sentences)\n",
        "sentence_tokenize = nltk.sent_tokenize(paragraph)\n",
        "\n",
        "\n",
        "# Lemmatization (looping through each sentence)\n",
        "for i in range(len(sentence_tokenize)):\n",
        "\n",
        " # Tokenize the sentence (split into individual words)\n",
        "  words = nltk.word_tokenize(sentence_tokenize[i])\n",
        "\n",
        "  # Remove stop words (common words with little meaning) and apply lemmatization\n",
        "  # (convert words to their base form while preserving meaning)\n",
        "  words = [WordNetLemmatizer().lemmatize(word) for word in words if word not in set(stopwords.words('english'))]\n",
        "\n",
        "  # Join the lemmatized words back into a complete sentence\n",
        "  sentence_tokenize[i] = ' '.join(words)"
      ],
      "metadata": {
        "id": "NqNUbDYJOQzj"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_tokenize[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "cFJCtXDMPzyU",
        "outputId": "545980b7-03c1-456a-e6f0-9b2b86774ee0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'If aim European war beginning nineteenth century aggrandizement Russia , aim might accomplished without preceding war without invasion .'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Creating the Bag of Words modle**"
      ],
      "metadata": {
        "id": "tO6sdtaAjJeM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cleaning the Text\n",
        "\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "\n",
        "# Create an object for stemming (reduces words to base form)\n",
        "PS = PorterStemmer()\n",
        "\n",
        "# Create an object for lemmatization (preserves meaning better)\n",
        "wordnet = WordNetLemmatizer()\n",
        "\n",
        "# Split the paragraph into sentences\n",
        "sentences = nltk.sent_tokenize(paragraph)\n",
        "\n",
        "# This list will store the cleaned sentences\n",
        "corpus = []\n",
        "\n",
        "\n",
        "for i in range(len(sentences)):\n",
        "\n",
        " # Remove non-alphanumeric characters (except whitespace)\n",
        "  review = re.sub('[^a-zA-Z]', ' ', sentences[i])\n",
        "\n",
        " # Convert the review text to lowercase for case-insensitive processing\n",
        "  review = review.lower()\n",
        "\n",
        " #  Split the review text into a list of individual words\n",
        "  review = review.split()\n",
        "\n",
        " # Remove common words (stopwords) that don't provide much meaning (e.g., \"the\", \"a\", \"is\")\n",
        " # Apply stemming (reduce words to their base form, may lose meaning)\n",
        "  review = [PS.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
        "\n",
        " # Join the cleaned words back into a sentence\n",
        "  cleaned_review = ' '.join(review)\n",
        "\n",
        " # Add the cleaned review to the corpus (collection of cleaned reviews)\n",
        "  corpus.append(cleaned_review)\n",
        "\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Create a CountVectorizer object\n",
        "# max_features=1500: This parameter limits the vocabulary size to 1500 most frequent words.\n",
        "# You can adjust this value depending on your needs (more words for richer representation, fewer words for faster processing).\n",
        "CV = CountVectorizer(max_features = 1500)\n",
        "\n",
        "# Fit the CountVectorizer to the corpus (collection of cleaned text documents)\n",
        "# This step analyzes the corpus and builds a vocabulary of the most frequent words.\n",
        "X = CV.fit_transform(corpus).toarray()"
      ],
      "metadata": {
        "id": "xI_nqjKIaSn7"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The resulting X is a document-term matrix:\n",
        "\n",
        "\n",
        "*   Each row represents a document in the corpus.\n",
        "*   Each column represents a word in the vocabulary (1500 most frequent words).\n",
        "*   The value at each cell (X[i, j]) represents the frequency (count) of word j in document i.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "odYBnQVn9LjS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tk1BqTXMtElB",
        "outputId": "7b6b81e6-0842-4df8-b660-eaa01cd57738"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, ..., 0, 1, 0],\n",
              "       [0, 0, 1, ..., 0, 2, 0],\n",
              "       [0, 0, 0, ..., 0, 2, 0],\n",
              "       ...,\n",
              "       [0, 0, 0, ..., 0, 0, 1],\n",
              "       [1, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Creating the TD-IDF modle**"
      ],
      "metadata": {
        "id": "n-VrH2jSjR1s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the paragraph into sentences\n",
        "sentences = nltk.sent_tokenize(paragraph)\n",
        "\n",
        "# This list will store the cleaned sentences\n",
        "corpus = []\n",
        "\n",
        "\n",
        "for i in range(len(sentences)):\n",
        "\n",
        " # Remove non-alphanumeric characters (except whitespace)\n",
        "  review = re.sub('[^a-zA-Z]', ' ', sentences[i])\n",
        "\n",
        " # Convert the review text to lowercase for case-insensitive processing\n",
        "  review = review.lower()\n",
        "\n",
        " # Split the review text into a list of individual words\n",
        "  review = review.split()\n",
        "\n",
        " # Remove common words (stopwords) that don't provide much meaning (e.g., \"the\", \"a\", \"is\")\n",
        " # Apply lemmatization (convert words to their base form while preserving meaning)\n",
        " # This is an alternative to stemming, which can sometimes create unrecognizable words.\n",
        "  review = [wordnet.lemmatize(word) for word in review if not word in set(stopwords.words('english'))]\n",
        "\n",
        "  # Join the cleaned words back into a sentence\n",
        "  cleaned_review = ' '.join(review)\n",
        "\n",
        " # Add the cleaned review to the corpus (collection of cleaned reviews)\n",
        "  corpus.append(cleaned_review)\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Create a TF-IDF vectorizer object\n",
        "tf = TfidfVectorizer()\n",
        "\n",
        "# This line performs two steps:\n",
        "# 1. Fit the TF-IDF vectorizer to the corpus (learn the vocabulary and weights)\n",
        "# 2. Transform the corpus into a document-term matrix (numerical features based on TF-IDF)\n",
        "X = tf.fit_transform(corpus).toarray()"
      ],
      "metadata": {
        "id": "KJzFjl8LaTjD"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fuNNccVqv8MB",
        "outputId": "fddf7ebd-fc08-48cf-81d9-53f19799d52a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.        , 0.        , ..., 0.        , 0.15735157,\n",
              "        0.        ],\n",
              "       [0.        , 0.        , 0.21295609, ..., 0.        , 0.38496643,\n",
              "        0.        ],\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.532069  ,\n",
              "        0.        ],\n",
              "       ...,\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.15017374],\n",
              "       [0.12527791, 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ]])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    }
  ]
}